{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch \n",
    "from random import randint, sample, seed\n",
    "from multiprocessing import Pool,cpu_count,current_process\n",
    "import numpy as np\n",
    "import multi_task as mt\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer, pipeline\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_prompts(dataset, num_examples=100):\n",
    "    if num_examples <= len(dataset):\n",
    "        num_examples=len(dataset)\n",
    "    picks = []\n",
    "    pick_indice=[]\n",
    "    for _ in range(num_examples):\n",
    "        pick = randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = randint(0, len(dataset)-1)\n",
    "        pick_indice.append(pick)\n",
    "        picks.append(dataset[pick])\n",
    "    return picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_dataset=load_dataset(\"allenai/real-toxicity-prompts\", split=\"train\")\n",
    "regard_dataset=load_dataset(\"AlexaAI/bold\",split=\"train\")\n",
    "honest_dataset=load_dataset(\"MilaNLProc/honest\",\"en_queer_nonqueer\",split=\"honest\")\n",
    "\n",
    "regard=evaluate.load(\"regard\",\"compare\")\n",
    "toxicity = evaluate.load(\"toxicity\")\n",
    "honest=evaluate.load(\"honest\",\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixtral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2da8dffd32c40b5bc24a74f5133afda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtral 7B Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1000)\n",
    "\n",
    "test_sample= sample_prompts(toxicity_dataset,1000)\n",
    "test_prompts = [p['prompt']['text'] for p in test_sample]\n",
    "test_sections= np.array_split(test_prompts,cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_continuations=[]\n",
    "argumnets=[[model,tokenizer,pbin,False] for pbin in test_sections]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.mixtral_generate,argumnets):\n",
    "    model_continuations=model_continuations+results\n",
    "\n",
    "display(len(model_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral_toxicty_rate=toxicity.compute(predictions=model_continuations, aggregation=\"ratio\")[\"toxicity_ratio\"]\n",
    "mixtral_toxicty=toxicity.compute(predictions=model_continuations)[\"toxicity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtral 7B Regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2000)\n",
    "\n",
    "female_bold=[b[\"prompts\"][0] for b in regard_dataset if b[\"category\"]==\"American_actresses\"]\n",
    "male_bold=[b[\"prompts\"][0] for b in regard_dataset if b[\"category\"]==\"American_actors\"]\n",
    "\n",
    "female_sample=sample_prompts(female_bold,500)\n",
    "male_sample=sample_prompts(male_bold,500)\n",
    "\n",
    "female_sections= np.array_split(female_sample,cpu_count())\n",
    "male_section= np.array_split(male_sample,cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_continuations=[]\n",
    "argumnets=[[model,tokenizer,pbin,False] for pbin in female_sections]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.mixtral_generate,argumnets):\n",
    "    female_continuations=female_continuations+results\n",
    "\n",
    "display(len(female_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_continuations=[]\n",
    "argumnets=[[model,tokenizer,pbin,False] for pbin in male_section]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.mixtral_generate,argumnets):\n",
    "    male_continuations=male_continuations+results\n",
    "\n",
    "display(len(male_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mixtral_regard_diff=regard.compute(data=male_continuations,references=female_continuations)[\"regard_difference\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtral Honest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(3000)\n",
    "\n",
    "honest_queer=[h[\"template_masked\"].replace(' [M].',\"\") for h in honest_dataset if h['category'].startswith('queer')]\n",
    "honest_nonqueer=[h[\"template_masked\"].replace(' [M].',\"\") for h in honest_dataset if h['category'].startswith('nonqueer')]\n",
    "\n",
    "queer_sample=sample_prompts(honest_queer,75)\n",
    "nonqueer_sample=sample_prompts(honest_nonqueer,75)\n",
    "\n",
    "queer_sections= np.array_split(queer_sample,cpu_count())\n",
    "nonqueer_section= np.array_split(nonqueer_sample,cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_continuations=[]\n",
    "argumnets=[[model,tokenizer,p,True] for p in queer_sections]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.mixtral_generate,argumnets):\n",
    "    queer_continuations=queer_continuations+results\n",
    "\n",
    "display(len(queer_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonquer_continuations=[]\n",
    "argumnets=[[model,tokenizer,p,True] for p in nonqueer_section]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.mixtral_generate,argumnets):\n",
    "    nonquer_continuations=nonquer_continuations+results\n",
    "\n",
    "display(len(nonquer_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=[\"queer\"]*50+[\"nonqueer\"]*50\n",
    "continuations=[q.split() for q in queer_continuations]+[nq.split() for nq in nonquer_continuations]\n",
    "\n",
    "Mixtral_honest=honest.compute(predictions=continuations,groups=groups)[\"honest_score_per_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi-2 Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1000)\n",
    "\n",
    "test_sample= sample_prompts(toxicity_dataset,1000)\n",
    "test_prompts = [p['prompt']['text'] for p in test_sample]\n",
    "test_sections= np.array_split(test_prompts,cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_continuations=[]\n",
    "argumnets=[[model,tokenizer,pbin,False] for pbin in test_sections]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.phi_generate,argumnets):\n",
    "    model_continuations=model_continuations+results\n",
    "\n",
    "display(len(model_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_toxicty_rate=toxicity.compute(predictions=model_continuations, aggregation=\"ratio\")[\"toxicity_ratio\"]\n",
    "phi_toxicty=toxicity.compute(predictions=model_continuations)[\"toxicity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi-2 Regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2000)\n",
    "\n",
    "female_bold=[b[\"prompts\"][0] for b in regard_dataset if b[\"category\"]==\"American_actresses\"]\n",
    "male_bold=[b[\"prompts\"][0] for b in regard_dataset if b[\"category\"]==\"American_actors\"]\n",
    "\n",
    "female_sample=sample_prompts(female_bold,500)\n",
    "male_sample=sample_prompts(male_bold,500)\n",
    "\n",
    "female_sections= np.array_split(female_sample,cpu_count())\n",
    "male_section= np.array_split(male_sample,cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_continuations=[]\n",
    "argumnets=[[model,tokenizer,pbin,False] for pbin in female_sections]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.phi_generate,argumnets):\n",
    "    female_continuations=female_continuations+results\n",
    "\n",
    "display(len(female_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_continuations=[]\n",
    "argumnets=[[model,tokenizer,pbin,False] for pbin in male_section]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.phi_generate,argumnets):\n",
    "    male_continuations=male_continuations+results\n",
    "\n",
    "display(len(male_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_regard_diff=regard.compute(data=male_continuations,references=female_continuations)[\"regard_difference\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi Honest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(3000)\n",
    "\n",
    "honest_queer=[h[\"template_masked\"].replace(' [M].',\"\") for h in honest_dataset if h['category'].startswith('queer')]\n",
    "honest_nonqueer=[h[\"template_masked\"].replace(' [M].',\"\") for h in honest_dataset if h['category'].startswith('nonqueer')]\n",
    "\n",
    "queer_sample=sample_prompts(honest_queer,75)\n",
    "nonqueer_sample=sample_prompts(honest_nonqueer,75)\n",
    "\n",
    "queer_sections= np.array_split(queer_sample,cpu_count())\n",
    "nonqueer_section= np.array_split(nonqueer_sample,cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_continuations=[]\n",
    "argumnets=[[model,tokenizer,p,True] for p in queer_sections]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.phi_generate,argumnets):\n",
    "    queer_continuations=queer_continuations+results\n",
    "\n",
    "display(len(queer_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonquer_continuations=[]\n",
    "argumnets=[[model,tokenizer,p,True] for p in nonqueer_section]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.phi_generate,argumnets):\n",
    "    nonquer_continuations=nonquer_continuations+results\n",
    "\n",
    "display(len(nonquer_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=[\"queer\"]*50+[\"nonqueer\"]*50\n",
    "continuations=[q.split() for q in queer_continuations]+[nq.split() for nq in nonquer_continuations]\n",
    "\n",
    "Phi_honest=honest.compute(predictions=continuations,groups=groups)[\"honest_score_per_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMa Pro 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"TencentARC/LLaMA-Pro-8B-Instruct\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TencentARC/LLaMA-Pro-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMa Pro Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1000)\n",
    "\n",
    "test_sample= sample_prompts(toxicity_dataset,1000)\n",
    "test_prompts = [p['prompt']['text'] for p in test_sample]\n",
    "test_sections= np.array_split(test_prompts,cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_continuations=[]\n",
    "argumnets=[[model,tokenizer,pbin,False] for pbin in test_sections]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.mixtral_generate,argumnets):\n",
    "    model_continuations=model_continuations+results\n",
    "\n",
    "display(len(model_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLaMa_toxicty_rate=toxicity.compute(predictions=model_continuations, aggregation=\"ratio\")[\"toxicity_ratio\"]\n",
    "LLaMa_toxicty=toxicity.compute(predictions=model_continuations)[\"toxicity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMa Regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2000)\n",
    "\n",
    "female_bold=[b[\"prompts\"][0] for b in regard_dataset if b[\"category\"]==\"American_actresses\"]\n",
    "male_bold=[b[\"prompts\"][0] for b in regard_dataset if b[\"category\"]==\"American_actors\"]\n",
    "\n",
    "female_sample=sample_prompts(female_bold,500)\n",
    "male_sample=sample_prompts(male_bold,500)\n",
    "\n",
    "female_sections= np.array_split(female_sample,cpu_count())\n",
    "male_section= np.array_split(male_sample,cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_continuations=[]\n",
    "argumnets=[[model,tokenizer,pbin,False] for pbin in female_sections]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.mixtral_generate,argumnets):\n",
    "    female_continuations=female_continuations+results\n",
    "\n",
    "display(len(female_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_continuations=[]\n",
    "argumnets=[[model,tokenizer,pbin,False] for pbin in male_section]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.mixtral_generate,argumnets):\n",
    "    male_continuations=male_continuations+results\n",
    "\n",
    "display(len(male_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLaMa_regard_diff=regard.compute(data=male_continuations,references=female_continuations)[\"regard_difference\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMa Honest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(3000)\n",
    "\n",
    "honest_queer=[h[\"template_masked\"].replace(' [M].',\"\") for h in honest_dataset if h['category'].startswith('queer')]\n",
    "honest_nonqueer=[h[\"template_masked\"].replace(' [M].',\"\") for h in honest_dataset if h['category'].startswith('nonqueer')]\n",
    "\n",
    "queer_sample=sample_prompts(honest_queer,75)\n",
    "nonqueer_sample=sample_prompts(honest_nonqueer,75)\n",
    "\n",
    "queer_sections= np.array_split(queer_sample,cpu_count())\n",
    "nonqueer_section= np.array_split(nonqueer_sample,cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_continuations=[]\n",
    "argumnets=[[model,tokenizer,p,True] for p in queer_sections]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.mixtral_generate,argumnets):\n",
    "    queer_continuations=queer_continuations+results\n",
    "\n",
    "display(len(queer_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonquer_continuations=[]\n",
    "argumnets=[[model,tokenizer,p,True] for p in nonqueer_section]\n",
    "with Pool(cpu_count(),maxtasksperchild=1) as p:\n",
    "  for results in p.starmap(mt.mixtral_generate,argumnets):\n",
    "    nonquer_continuations=nonquer_continuations+results\n",
    "\n",
    "display(len(nonquer_continuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=[\"queer\"]*50+[\"nonqueer\"]*50\n",
    "continuations=[q.split() for q in queer_continuations]+[nq.split() for nq in nonquer_continuations]\n",
    "\n",
    "LLaMa_toxicty_rate_honest=honest.compute(predictions=continuations,groups=groups)[\"honest_score_per_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
