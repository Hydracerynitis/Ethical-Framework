# Evaluate

## Experiment

Waiting for Server access

## Comment

### Generality

- Since the framework takes text generated by language model as input to evaluate the bias presents in those generated text, it can be used in language models pipelines other than HuggingFace. It makes it a very flexible framework.
- Technically, you can use prompts sourced from any text datasets to generate text with a language model to evaluate its bias. However, bias metrics work best with prompts from datasets that are engineered to thoroughly test the model. 
    - **Toxicity** metrics are paried with datasets with toxicity prompts
    - **Regard** metrics are paired with **BOLD** datasets
    - **Honest** metrics are paired with **Honest** datasets

### Stability/Reliability

- Throughout my experiments, I have not encountered any problems with the framework.

### Guidance

- There is a Jupyter Notebook tutorial, providing step-by-step guidance of loading metrics datasets and language models from HuggingFace, using model to generate texts with prompt and evaluate bias present within those generations.
- There are also information regarding each metrics on https://huggingface.co/evaluate-measurement. Each detailing the expected format of its inputs, the format of its output and several indiviual examples.

## Advantage and Limitation

- The framework can be regarded as a complete package. It means that it can be integrated into any language model workflows.
- Despite providing a range of metrics to evaluate a language modle, the framework provided limited bias metricses. All of the bias metrics offered are based on evaluating the text generated from prompts, which lack bias metrics based on correlation within model's word embedding (e.g. WEAT and WEFAT). 
    - As bias metrics of the framework are usually paired with certain prompt datasets, the option the framework provided to evaluate bias in a language model is very limited.