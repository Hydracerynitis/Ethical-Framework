# Bias Evaluation Framework Survey

Studies have found evidence that machine learning models, despite very accurate, can propagate bias present in the datasets through their predictions. To solve the bias problem, fairness machine learning is a new research field focusing on quantifying bias of models' prediction and designing methods to mitigate those bias without trading too much accuracy. Bias Evaluation Frameworks are one of the product of fairness machine learning. They offer tools to calculate fairness metrics of models, provide bias mitigation implementation, and sometime automate benchmark experiments. My survey is to examine the current state of bias evaluation frameworks on tabular models and language models, and identify any problems that their future mass adaption may encounter. For each framework, I learn to utilize it, incorporate it into a mock up experiment, and I evaluate its strength and weakness based on my personal experience. My main finding is that common problems that all frameworks in the survey have are that their roster of fairness metrics are limited to evaluate the relative correctness of predictions across protected and privileged groups. In addition They also lack guidance for new users to understand the purpose and usage of different metrics.