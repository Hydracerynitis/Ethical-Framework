# Bias Evaluation Framework Survey (Expert Audience)

Studies have found evidence that despite being very accurate, the machine learning model can propagate biases present in the datasets through their predictions. Fairness machine learning has become an emerging research field to solve the bias problem. It focuses on quantifying the bias of models' predictions and designing methods to mitigate those biases without trading too much accuracy. Bias evaluation frameworks are one of the products of fairness machine learning. They offer tools to calculate fairness metrics of models, provide bias mitigation implementations, and sometimes automate benchmark experiments. My survey examines the current state of bias evaluation frameworks on tabular and language models and identifies any problems their future mass adaption may encounter. For each framework, I learned to incorporate it into a mockup experiment and evaluate its strengths and weaknesses based on my personal experience. My main finding is that a common problem that all frameworks in the survey have is that their roster of fairness metrics is limited to evaluate the relative correctness of predictions across protected and privileged groups. In addition, they also lack guidance for new users to understand the purpose and usage of different metrics.

# Fairness Evaluation Framework Survey (Educated, but Non-expert Audience)

AI technologies, such as tabular and language models, have become increasingly prevalent in the day-to-day decision-making process, all thanks to their accuracy and knowledgeableness. However, studies have found evidence that those models risk making discriminatory decisions due to bias in their training data. Fairness machine learning emerged as a new research field in the machine learning discipline to address this fairness problem. Fairness evaluation frameworks are one of the research field's products, providing tools to quantify bias a model may express with fairness metrics and sometimes offering methods to help reduce such bias. My survey examines both models' current state of bias evaluation frameworks and identifies any problems their future mass adaption may encounter. For each framework, I learned to incorporate it into a mockup experiment and evaluate its strengths and weaknesses based on my personal experience. My main finding is that all frameworks in the survey offer a limited view of fairness, regarding it as the sameness of treatment (i.e. equality). As a result, they may not be able to cover the situations where proportional fairness (i.e. equity) is more relevant.

# Bias Evaluation Framework Survey (Merged version)

Recently, technologies based on artificial intelligence (AI) have become increasingly prevalent in the day-to-day decision-making process. However, studies have found evidence that those models have the risk of making discriminatory decisions. Bias in AI can occur at various stages, from data collection to algorithmic development and deployment. This survey focuses on understanding the evaluation frameworks, one aspect of addressing the bias problem. Bias evaluation frameworks offer tools to calculate fairness metrics of models, provide bias mitigation implementations, and sometimes automate benchmark experiments. This survey examines the current state of bias evaluation frameworks on tabular and language models and identifies any problems their future mass adaption may encounter. Each framework is incorporated into a mock-up experiment, and the strengths and weaknesses are evaluated based on personal experience. The main finding is that for all frameworks in the survey, their roster of fairness metrics is limited to assessing the relative correctness of predictions across protected and privileged groups. In addition, they also lack guidance for new users to understand the purpose and usage of different metrics. 